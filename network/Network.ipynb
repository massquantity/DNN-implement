{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/massquantity/Workspace/neural-networks-and-deep-learning/mine'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('/home/massquantity/Workspace/neural-networks-and-deep-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/usr/lib/spark/spark-2.3.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip',\n",
       " '/usr/lib/spark/spark-2.3.2-bin-hadoop2.7/python',\n",
       " '/home/massquantity/Workspace/neural-networks-and-deep-learning/mine',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python35.zip',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5/plat-linux',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5/lib-dynload',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5/site-packages',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5/site-packages/python_recsys-0.2-py3.5.egg',\n",
       " '/home/massquantity/.conda/envs/py35/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/home/massquantity/.ipython',\n",
       " '/home/massquantity/Workspace/neural-networks-and-deep-learning']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import mnist_loader\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "\n",
    "    def feedward(self, a):\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = Network.relu(np.dot(w, a) + b)  ####\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            for k in range(0, len(training_data), mini_batch_size):\n",
    "                mini_batch = training_data[k : k+mini_batch_size]\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j+1, self.evaluate(test_data), len(test_data)))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j+1))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            add_b, add_w = self.backprop(x, y)\n",
    "            gradient_b = [gb+ab for gb, ab in zip(gradient_b, add_b)]\n",
    "            gradient_w = [gw+aw for gw, aw in zip(gradient_w, add_w)]\n",
    "\n",
    "        self.biases = [bias - eta * gb / len(mini_batch) for bias, gb in zip(self.biases, gradient_b)]\n",
    "        self.weights = [weight - eta * gw / len(mini_batch) for weight, gw in zip(self.weights, gradient_w)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        ## forward\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = Network.relu(z)   #####\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]  ####\n",
    "        z_hold.append(final_layer)                                 ####\n",
    "        a_hold.append(Network.softmax(final_layer))               #####\n",
    "\n",
    "        ## backward\n",
    "        delta = self.cost_derivative(a_hold[-1], y) # * self.relu_derivative(z_hold[-1])   ####\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * Network.relu_derivative(z_hold[-l])   ### \n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l-1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_b, gradient_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedward(x)), y) for x, y in test_data]\n",
    "        return sum(int(x == y) for x, y in test_results)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def cost_derivative(x, y):\n",
    "        return x - y\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        max_val = np.max(z)\n",
    "        z = z - max_val\n",
    "        return np.exp(z) / np.sum(np.exp(z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x) # 溢出对策\n",
    "        return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 7745 / 10000\n",
      "Epoch 2: 7949 / 10000\n",
      "Epoch 3: 8591 / 10000\n",
      "Epoch 4: 8321 / 10000\n",
      "Epoch 5: 8731 / 10000\n",
      "Epoch 6: 8904 / 10000\n",
      "Epoch 7: 8560 / 10000\n",
      "Epoch 8: 8972 / 10000\n",
      "Epoch 9: 8785 / 10000\n",
      "Epoch 10: 9074 / 10000\n",
      "Epoch 11: 9018 / 10000\n",
      "Epoch 12: 9082 / 10000\n",
      "Epoch 13: 9117 / 10000\n",
      "Epoch 14: 9052 / 10000\n",
      "Epoch 15: 9174 / 10000\n",
      "Epoch 16: 9191 / 10000\n",
      "Epoch 17: 9188 / 10000\n",
      "Epoch 18: 9188 / 10000\n",
      "Epoch 19: 9175 / 10000\n",
      "Epoch 20: 9205 / 10000\n",
      "Epoch 21: 9205 / 10000\n",
      "Epoch 22: 9284 / 10000\n",
      "Epoch 23: 9284 / 10000\n",
      "Epoch 24: 9306 / 10000\n",
      "Epoch 25: 9324 / 10000\n",
      "Epoch 26: 9264 / 10000\n",
      "Epoch 27: 9322 / 10000\n",
      "Epoch 28: 9315 / 10000\n",
      "Epoch 29: 9358 / 10000\n",
      "Epoch 30: 9373 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 32, 0.1, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for data, target in training_data:\n",
    "    X.append(data)\n",
    "    y.append(target)\n",
    "\n",
    "X = np.array(X).reshape(50000, -1)\n",
    "y = np.array(y).reshape(50000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000, 10))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for data, target in test_data:\n",
    "    test_X.append(data)\n",
    "    test_y.append(target)\n",
    "\n",
    "test_X = np.array(test_X).reshape(10000, -1)\n",
    "test_y = np.array(test_y).reshape(10000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 1))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weights = [np.random.randn(forward_layer, back_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer) for layer in sizes[1:]]\n",
    "\n",
    "    def feedward(self, a):\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = Network.relu(np.dot(a, w) + b)  ####\n",
    "        a = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "   #     a = Network.softmax(a)  #####\n",
    "        return a\n",
    "\n",
    "    def SGD(self, X, y, epochs, mini_batch_size, eta, test_X=None, test_y=None):\n",
    "    #     training_data = list(training_data)\n",
    "    #     if test_data:\n",
    "    #         test_data = list(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random_mask = np.random.choice(len(X), len(X), replace=False)\n",
    "            X = X[random_mask]\n",
    "            y = y[random_mask]\n",
    "            \n",
    "            for k in range(0, len(X), mini_batch_size):\n",
    "                X_batch = X[k : k + mini_batch_size]\n",
    "                y_batch = y[k : k + mini_batch_size]\n",
    "                self.update_mini_batch(X_batch, y_batch, eta)\n",
    "                \n",
    "       #     print(\"Tringing Epoch {0}: {1} / {2}\".format(j+1, self.evaluate(X, y), len(X)))\n",
    "            if test_X is not None:\n",
    "                print(\"Epoch {0}: {1}\".format(j+1, self.evaluate(test_X, test_y)))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j+1))\n",
    "\n",
    "    def update_mini_batch(self, X_batch, y_batch, eta):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        add_b, add_w = self.backprop(X_batch, y_batch)\n",
    "        gradient_b = [gb+ab for gb, ab in zip(gradient_b, add_b)]\n",
    "        gradient_w = [gw+aw for gw, aw in zip(gradient_w, add_w)]\n",
    "\n",
    "        self.biases = [bias - eta * gb / len(X_batch) for bias, gb in zip(self.biases, gradient_b)]\n",
    "        self.weights = [weight - eta * gw / len(X_batch) for weight, gw in zip(self.weights, gradient_w)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        ## forward\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(a, w) + b\n",
    "            a = Network.relu(z)   #####\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(a, self.weights[-1]) + self.biases[-1]  ####\n",
    "        z_hold.append(final_layer)                                 ####\n",
    "        a_hold.append(Network.softmax(final_layer))               #####\n",
    "\n",
    "        ## backward\n",
    "        delta = self.cost_derivative(a_hold[-1], y) # * self.relu_derivative(z_hold[-1])   ####\n",
    "        gradient_w[-1] = np.dot(a_hold[-2].T, delta)\n",
    "        gradient_b[-1] = np.sum(delta, axis=0)   ########\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(delta, self.weights[-l+1].T) * Network.relu_derivative(z_hold[-l])   ### \n",
    "            gradient_w[-l] = np.dot(a_hold[-l-1].T, delta)\n",
    "            gradient_b[-l] = np.sum(delta, axis=0)   ##  ###\n",
    "            \n",
    "        return gradient_b, gradient_w\n",
    "\n",
    "    def evaluate(self, test_X, test_y):\n",
    "        test_results = np.argmax(self.feedward(test_X), axis=1)\n",
    "        test_y = test_y.reshape(10000)  #####\n",
    "        accuracy = np.sum(test_results == test_y) / float(test_X.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def cost_derivative(x, y):\n",
    "        return x - y\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        max_val = np.max(z)\n",
    "        z_max = z - max_val\n",
    "        return np.exp(z) / np.sum(np.exp(z)) '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        if x.ndim == 2:\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            return y.T \n",
    "\n",
    "        x = x - np.max(x) # 溢出对策\n",
    "        return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0.6719\n",
      "Epoch 2: 0.7094\n",
      "Epoch 3: 0.7369\n",
      "Epoch 4: 0.7507\n",
      "Epoch 5: 0.7777\n",
      "Epoch 6: 0.7892\n",
      "Epoch 7: 0.792\n",
      "Epoch 8: 0.8057\n",
      "Epoch 9: 0.7782\n",
      "Epoch 10: 0.8122\n",
      "Epoch 11: 0.8177\n",
      "Epoch 12: 0.8184\n",
      "Epoch 13: 0.8132\n",
      "Epoch 14: 0.8251\n",
      "Epoch 15: 0.8248\n",
      "Epoch 16: 0.8415\n",
      "Epoch 17: 0.842\n",
      "Epoch 18: 0.84\n",
      "Epoch 19: 0.8494\n",
      "Epoch 20: 0.8507\n",
      "Epoch 21: 0.8464\n",
      "Epoch 22: 0.846\n",
      "Epoch 23: 0.8575\n",
      "Epoch 24: 0.8623\n",
      "Epoch 25: 0.8515\n",
      "Epoch 26: 0.8633\n",
      "Epoch 27: 0.8625\n",
      "Epoch 28: 0.8584\n",
      "Epoch 29: 0.8621\n",
      "Epoch 30: 0.869\n",
      "Epoch 31: 0.8737\n",
      "Epoch 32: 0.863\n",
      "Epoch 33: 0.8735\n",
      "Epoch 34: 0.8679\n",
      "Epoch 35: 0.8772\n",
      "Epoch 36: 0.8766\n",
      "Epoch 37: 0.8742\n",
      "Epoch 38: 0.8739\n",
      "Epoch 39: 0.8734\n",
      "Epoch 40: 0.8801\n",
      "Epoch 41: 0.8808\n",
      "Epoch 42: 0.8825\n",
      "Epoch 43: 0.8825\n",
      "Epoch 44: 0.886\n",
      "Epoch 45: 0.8817\n",
      "Epoch 46: 0.8755\n",
      "Epoch 47: 0.8915\n",
      "Epoch 48: 0.8882\n",
      "Epoch 49: 0.8908\n",
      "Epoch 50: 0.8868\n",
      "Epoch 51: 0.8897\n",
      "Epoch 52: 0.8941\n",
      "Epoch 53: 0.891\n",
      "Epoch 54: 0.8851\n",
      "Epoch 55: 0.8938\n",
      "Epoch 56: 0.8969\n",
      "Epoch 57: 0.8921\n",
      "Epoch 58: 0.8934\n",
      "Epoch 59: 0.8991\n",
      "Epoch 60: 0.8954\n",
      "Epoch 61: 0.9008\n",
      "Epoch 62: 0.8987\n",
      "Epoch 63: 0.8969\n",
      "Epoch 64: 0.8977\n",
      "Epoch 65: 0.892\n",
      "Epoch 66: 0.901\n",
      "Epoch 67: 0.9021\n",
      "Epoch 68: 0.902\n",
      "Epoch 69: 0.8971\n",
      "Epoch 70: 0.9048\n",
      "Epoch 71: 0.8984\n",
      "Epoch 72: 0.9012\n",
      "Epoch 73: 0.9004\n",
      "Epoch 74: 0.9048\n",
      "Epoch 75: 0.9025\n",
      "Epoch 76: 0.9069\n",
      "Epoch 77: 0.9049\n",
      "Epoch 78: 0.9024\n",
      "Epoch 79: 0.9032\n",
      "Epoch 80: 0.9095\n",
      "Epoch 81: 0.9022\n",
      "Epoch 82: 0.9091\n",
      "Epoch 83: 0.9106\n",
      "Epoch 84: 0.9005\n",
      "Epoch 85: 0.9096\n",
      "Epoch 86: 0.909\n",
      "Epoch 87: 0.91\n",
      "Epoch 88: 0.9144\n",
      "Epoch 89: 0.9137\n",
      "Epoch 90: 0.906\n",
      "Epoch 91: 0.9102\n",
      "Epoch 92: 0.9109\n",
      "Epoch 93: 0.9127\n",
      "Epoch 94: 0.9091\n",
      "Epoch 95: 0.9089\n",
      "Epoch 96: 0.9169\n",
      "Epoch 97: 0.9124\n",
      "Epoch 98: 0.9178\n",
      "Epoch 99: 0.9145\n",
      "Epoch 100: 0.9174\n"
     ]
    }
   ],
   "source": [
    "net.SGD(X, y, 100, 32, 0.01, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# momentum， Adam\n",
    "\n",
    "class Network:  \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer, 1) for layer in sizes[1:]]\n",
    "   #     self.velocity_w = [np.zeros(w.shape) for w in self.weights]\n",
    "   #     self.velocity_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    \n",
    "        self.vw = [np.zeros(w.shape) for w in self.weights] \n",
    "        self.hw = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.vb = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.hb = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.iter = 0\n",
    "\n",
    "\n",
    "    def feedward(self, a):\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = Network.relu(np.dot(w, a) + b)  ####\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def Adam(self, training_data, epochs, mini_batch_size, eta, test_data=None):  ##### SGD\n",
    "        training_data = list(training_data)\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            for k in range(0, len(training_data), mini_batch_size):\n",
    "                mini_batch = training_data[k : k+mini_batch_size]\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(j+1, self.evaluate(test_data), len(test_data)))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j+1))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, momentum=0.9, rho1=0.9, rho2=0.999):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            add_b, add_w = self.backprop(x, y)\n",
    "            gradient_b = [gb+ab for gb, ab in zip(gradient_b, add_b)]\n",
    "            gradient_w = [gw+aw for gw, aw in zip(gradient_w, add_w)]\n",
    "            \n",
    "     #   self.velocity_w = [momentum * vw - eta * gw / len(mini_batch) for vw, gw in zip(self.velocity_w, gradient_w)]\n",
    "     #   self.velocity_b = [momentum * vb - eta * gb / len(mini_batch) for vb, gb in zip(self.velocity_b, gradient_b)]\n",
    "\n",
    "     #   self.biases = [bias + vb for bias, vb in zip(self.biases, self.velocity_b)]\n",
    "     #   self.weights = [weight + vw for weight, vw in zip(self.weights, self.velocity_w)]\n",
    "        \n",
    "        self.iter += 1\n",
    "        gradient_w = [gw / len(mini_batch) for gw in gradient_w]\n",
    "        gradient_b = [gb / len(mini_batch) for gb in gradient_b]\n",
    "        \n",
    "        self.vw = [rho1 * vw + (1 - rho1) * gw for vw, gw in zip(self.vw, gradient_w)]\n",
    "        self.vb = [rho1 * vb + (1 - rho1) * gb for vb, gb in zip(self.vb, gradient_b)]\n",
    "        self.hw = [rho2 * hw + (1 - rho2) * (gw ** 2) for hw, gw in zip(self.hw, gradient_w)]\n",
    "        self.hb = [rho2 * hb + (1 - rho2) * (gb ** 2) for hb, gb in zip(self.hb, gradient_b)]\n",
    "        unbias_vw = [vw / (1 - rho1 ** self.iter) for vw in self.vw]\n",
    "        unbias_vb = [vb / (1 - rho1 ** self.iter) for vb in self.vb]\n",
    "        unbias_hw = [hw / (1 - rho2 ** self.iter) for hw in self.hw]\n",
    "        unbias_hb = [hb / (1 - rho2 ** self.iter) for hb in self.hb]\n",
    "        self.weights = [weight - eta * vw / (np.sqrt(hw) + 1e-8) for weight, vw, hw in zip(self.weights, unbias_vw, unbias_hw)]\n",
    "        self.biases = [bias - eta * vb / (np.sqrt(hb) + 1e-8) for bias, vb, hb in zip(self.biases, unbias_vb, unbias_hb)]\n",
    "        \n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        ## forward\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = Network.relu(z)   #####\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]  ####\n",
    "        z_hold.append(final_layer)                                 ####\n",
    "        a_hold.append(Network.softmax(final_layer))               #####\n",
    "\n",
    "        ## backward\n",
    "        delta = self.cost_derivative(a_hold[-1], y) # * self.relu_derivative(z_hold[-1])   ####\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * Network.relu_derivative(z_hold[-l])   ### \n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l-1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_b, gradient_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedward(x)), y) for x, y in test_data]\n",
    "        return sum(int(x == y) for x, y in test_results)\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def cost_derivative(x, y):\n",
    "        return x - y\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        max_val = np.max(z)\n",
    "        z_max = z - max_val\n",
    "        return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.SGD(training_data, 30, 32, 0.1, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 5623 / 10000\n",
      "Epoch 2: 7180 / 10000\n",
      "Epoch 3: 7986 / 10000\n",
      "Epoch 4: 8368 / 10000\n",
      "Epoch 5: 8641 / 10000\n",
      "Epoch 6: 8844 / 10000\n",
      "Epoch 7: 8986 / 10000\n",
      "Epoch 8: 9080 / 10000\n",
      "Epoch 9: 9157 / 10000\n",
      "Epoch 10: 9198 / 10000\n",
      "Epoch 11: 9247 / 10000\n",
      "Epoch 12: 9256 / 10000\n",
      "Epoch 13: 9325 / 10000\n",
      "Epoch 14: 9346 / 10000\n",
      "Epoch 15: 9373 / 10000\n",
      "Epoch 16: 9381 / 10000\n",
      "Epoch 17: 9417 / 10000\n",
      "Epoch 18: 9407 / 10000\n",
      "Epoch 19: 9427 / 10000\n",
      "Epoch 20: 9453 / 10000\n",
      "Epoch 21: 9461 / 10000\n",
      "Epoch 22: 9436 / 10000\n",
      "Epoch 23: 9455 / 10000\n",
      "Epoch 24: 9481 / 10000\n",
      "Epoch 25: 9476 / 10000\n",
      "Epoch 26: 9487 / 10000\n",
      "Epoch 27: 9477 / 10000\n",
      "Epoch 28: 9458 / 10000\n",
      "Epoch 29: 9486 / 10000\n",
      "Epoch 30: 9512 / 10000\n"
     ]
    }
   ],
   "source": [
    "net.Adam(training_data, 30, 32, 0.001, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
