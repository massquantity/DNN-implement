{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with gzip.open(\"../data/mnist.pkl.gz\", 'rb') as f:\n",
    "        train_data, val_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    train_inputs = [np.reshape(x, (28 * 28, 1)) for x in train_data[0]] + \\\n",
    "                   [np.reshape(x, (28 * 28, 1)) for x in val_data[0]]\n",
    "    train_labels = [vectorized_result(y) for y in train_data[1]] + \\\n",
    "                   [vectorized_result(y) for y in val_data[1]]\n",
    "   # valid_inputs = [np.reshape(x, (28*28, 1)) for x in val_data[0]]\n",
    "   # valid_labels = [vectorized_result(y) for y in val_data[1]]\n",
    "    test_inputs = [np.reshape(x, (28*28, 1)) for x in test_data[0]]\n",
    "    test_labels = [vectorized_result(y) for y in test_data[1]]\n",
    "    return (train_inputs, train_labels), (test_inputs, test_labels)  # (valid_inputs, valid_labels),\n",
    "\n",
    "def vectorized_result(i):\n",
    "    label = np.zeros((10, 1))\n",
    "    label[i] = 1.0\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x1, y1), (t1, h1) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "key_files = ['train-images-idx3-ubyte.gz',\n",
    "            'train-labels-idx1-ubyte.gz',\n",
    "            't10k-images-idx3-ubyte.gz',\n",
    "            't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "save_file = os.path.join(\"mnist_download/\", \"mnist.pkl\")\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist_download/mnist.pkl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist_download/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    data = np.frombuffer(f.read(), np.uint8, offset=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47040000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0] / 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = []\n",
    "for i in range(0, len(data), 784):\n",
    "    data_p.append(np.reshape(data[i: i+784], (784, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_p[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('mnist_download/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    data = np.frombuffer(f.read(), np.uint8, offset=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), array([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "for i in range(len(data)):\n",
    "    T = np.zeros((10, 1))\n",
    "    T[data[i]] = 1.0\n",
    "    ys.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]), array([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, sys, random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cifar10/data_batch_1', 'rb') as f:\n",
    "    aa = pickle.load(f, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'labels', b'data', b'batch_label', b'filenames'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "for i in range(1, 6):\n",
    "    with open('cifar10/data_batch_%d' % i, 'rb') as f:\n",
    "        whole = pickle.load(f, encoding='bytes')\n",
    "        data.extend(whole[b'data'])\n",
    "        labels.extend(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = [], []\n",
    "with open('cifar10/test_batch', 'rb') as f:\n",
    "    whole = pickle.load(f, encoding='bytes')\n",
    "    test_data = whole[b'data']\n",
    "    test_labels = np.array(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [np.reshape(row, (3072, 1)) for row in data]\n",
    "X_test = [np.reshape(row, (3072, 1)) for row in test_data]\n",
    "two_dim = np.eye(10)[np.array(labels, dtype=np.int32)]\n",
    "two_dim_test = np.eye(10)[np.array(test_labels, dtype=np.int32)]\n",
    "y_train = [row.reshape(10, 1) for row in two_dim]\n",
    "y_test = [row.reshape(10, 1) for row in two_dim_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\", dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param sizes: list of layers\n",
    "        :param activations: activation_functions\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer) * np.sqrt(2.0 / forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(back_layer, 1) for back_layer in sizes[1:]]\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # TODO  activation_functions = {'sigmoid': sigmoid, 'relu': relu} tanh\n",
    "        if activation.lower() == \"sigmoid\":\n",
    "            self.activation = Network.sigmoid\n",
    "            self.activation_derivative = Network.sigmoid_derivative\n",
    "        elif activation.lower() == \"relu\":\n",
    "            self.activation = Network.relu\n",
    "            self.activation_derivative = Network.relu_derivative\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(w, a) + b)\n",
    "            a *= (1.0 - self.dropout_rate)  ######### test dropout\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # forward pass #\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "\n",
    "            self.mask = np.random.rand(*z.shape) > self.dropout_rate\n",
    "            z *= self.mask\n",
    "        #    z /= (1 - self.dropout_rate)\n",
    "\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(Network.softmax(final_layer))\n",
    "\n",
    "        # backward pass#\n",
    "        delta = Network.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l - 1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_w, gradient_b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = z - np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_batch(z):\n",
    "        z = z.T\n",
    "        z = z - np.max(z, axis=0)\n",
    "        t = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        return t.T\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(a, b):\n",
    "        return a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DNN(X_train, y_train, num_epochs, learning_rate, network, X_val=None, y_val=None):\n",
    "    gradient_w = [np.zeros(w.shape) for w in network.weights]\n",
    "    gradient_b = [np.zeros(b.shape) for b in network.biases]\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        index = np.arange(len(X_train))\n",
    "        random.shuffle(index)\n",
    "     #   random_mask = np.random.choice(len(X_train), len(X_train), replace=False)\n",
    "        for idx in index:\n",
    "            add_w, add_b = network.backprop(X_train[idx], y_train[idx])\n",
    "            network.weights = [weight - learning_rate * gw for weight, gw in zip(network.weights, add_w)]\n",
    "            network.biases = [bias - learning_rate * gb for bias, gb in zip(network.biases, add_b)]\n",
    "\n",
    "         #   gradient_w = [gw + aw for gw, aw in zip(gradient_w, add_w)]\n",
    "         #   gradient_b = [gb + ab for gb, ab in zip(gradient_b, add_b)]\n",
    "         #   network.weights = [weight - learning_rate * gw for weight, gw in zip(network.weights, gradient_w)]\n",
    "        #    network.biases = [bias - learning_rate * gb for bias, gb in zip(network.biases, gradient_b)]\n",
    "\n",
    "        if X_val:\n",
    "            print(\"Epoch {0}, training_accuracy: {1},\\t validation accuracy: {2}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network), evaluate(X_val, y_val, network)))\n",
    "        else:\n",
    "            print(\"Epoch {0}, training_accuracy: {1}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network)))\n",
    "\n",
    "def evaluate(X_val, y_val, network):\n",
    "    y_pred = [np.argmax(network.predict(x)) for x in X_val]\n",
    "    return np.mean([int(y_p == np.argmax(y)) for y_p, y in zip(y_pred, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.1,\t validation accuracy: 0.1\n",
      "Epoch 2, training_accuracy: 0.1,\t validation accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "dnn = Network(sizes=[3072, 50, 10], activation=\"relu\", dropout_rate=0.5)\n",
    "train_DNN(X_train, y_train, 30, 0.01, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
