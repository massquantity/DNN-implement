{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import pickle, sys, random\n",
    "import gzip, time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.pardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data.mnist_1 import load_data\n",
    "from data.mnist_2 import load_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\", dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param sizes: list of layers\n",
    "        :param activations: activation_functions\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer) * np.sqrt(2.0 / forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(back_layer, 1) for back_layer in sizes[1:]]\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # TODO  activation_functions = {'sigmoid': sigmoid, 'relu': relu} tanh\n",
    "        if activation.lower() == \"sigmoid\":\n",
    "            self.activation = Network.sigmoid\n",
    "            self.activation_derivative = Network.sigmoid_derivative\n",
    "        elif activation.lower() == \"relu\":\n",
    "            self.activation = Network.relu\n",
    "            self.activation_derivative = Network.relu_derivative\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(w, a) + b)\n",
    "            a *= (1.0 - self.dropout_rate)  ######### test dropout\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # forward pass #\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "\n",
    "            self.mask = np.random.rand(*z.shape) > self.dropout_rate\n",
    "            z *= self.mask\n",
    "        #    z /= (1 - self.dropout_rate)\n",
    "\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(Network.softmax(final_layer))\n",
    "\n",
    "        # backward pass#\n",
    "        delta = Network.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l - 1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_w, gradient_b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = z - np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_batch(z):\n",
    "        z = z.T\n",
    "        z = z - np.max(z, axis=0)\n",
    "        t = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        return t.T\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(a, b):\n",
    "        return a - b\n",
    "\n",
    "\n",
    "class Network_mini_batch(Network):\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\"):\n",
    "        super().__init__(sizes, activation)\n",
    "        self.weights = [np.random.randn(forward_layer, back_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer) for layer in sizes[1:]]\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(a, w) + b)\n",
    "        a = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(a, w) + b  # batch  z = a * w + b\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(self.softmax_batch(final_layer))\n",
    "        \n",
    "        delta = self.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(a_hold[-2].T, delta)\n",
    "        gradient_b[-1] = np.sum(delta, axis=0)\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(delta, self.weights[-l + 1].T) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(a_hold[-l - 1].T, delta)\n",
    "            gradient_b[-l] = np.sum(delta, axis=0)\n",
    "\n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_DNN_minibatch(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test=None, y_test=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        random_mask = np.random.choice(len(X_train), len(X_train), replace=False)\n",
    "        X_train = X_train[random_mask]\n",
    "        y_train = y_train[random_mask]\n",
    "\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i : i + batch_size]\n",
    "            y_batch = y_train[i : i + batch_size]\n",
    "            gradient_w = [np.zeros(w.shape) for w in network.weights]\n",
    "            gradient_b = [np.zeros(b.shape) for b in network.biases]\n",
    "            add_w, add_b = network.backprop(X_batch, y_batch)\n",
    "            gradient_w = [gw + aw for gw, aw in zip(gradient_w, add_w)]\n",
    "            gradient_b = [gb + ab for gb, ab in zip(gradient_b, add_b)]\n",
    "            network.weights = [weight - learning_rate * gw / batch_size for weight, gw in zip(network.weights, gradient_w)]\n",
    "            network.biases = [bias - learning_rate * gb / batch_size for bias, gb in zip(network.biases, gradient_b)]\n",
    "\n",
    "        if X_test is not None:\n",
    "            print(\"Epoch {0}, training_accuracy: {1},\\t validation accuracy: {2}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network), evaluate(X_test, y_test, network)))\n",
    "        else:\n",
    "            print(\"Epoch {0}, training_accuracy: {1}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network)))\n",
    "\n",
    "def evaluate(X_val, y_val, network):\n",
    "    y_pred = [np.argmax(network.predict(x)) for x in X_val]\n",
    "    return np.mean([int(y_p == np.argmax(y)) for y_p, y in zip(y_pred, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_, n_classes=10):  ####\n",
    "    # Function to encode neural one-hot output labels from number indexes\n",
    "    # e.g.:\n",
    "    # one_hot(y_=[[5], [0], [3]], n_classes=6):\n",
    "    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n",
    "\n",
    "def load_data_2(normalize=True, flatten=True, one_hot=True, batch=True):\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "\n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    if normalize:\n",
    "        for t in [0, 2]:\n",
    "    #        dataset[t] = dataset[t].astype(np.float32)\n",
    "     #       dataset[t] /= 255.0\n",
    "            dataset[t] = [x.astype(np.float32) / 255.0 for x in dataset[t]]\n",
    "\n",
    "    if one_hot:\n",
    "        for y in [1, 3]:\n",
    "            dataset[y] = one_hot(dataset[y])\n",
    "\n",
    "    if batch:\n",
    "        for i in range(4):\n",
    "            length = len(dataset[i])\n",
    "            dataset[i] = np.array(dataset[i]).reshape(length, -1)\n",
    "\n",
    "    if not flatten:\n",
    "        for t in [0, 2]:\n",
    "            dataset[t] = dataset[t].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset[0], dataset[1]), (dataset[2], dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data_2(batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.6496666666666666,\t validation accuracy: 0.6625\n",
      "Epoch 2, training_accuracy: 0.6859333333333333,\t validation accuracy: 0.6963\n",
      "Epoch 3, training_accuracy: 0.7047,\t validation accuracy: 0.7151\n",
      "Epoch 4, training_accuracy: 0.7294,\t validation accuracy: 0.7398\n",
      "Epoch 5, training_accuracy: 0.7442333333333333,\t validation accuracy: 0.7566\n",
      "Epoch 6, training_accuracy: 0.7512333333333333,\t validation accuracy: 0.7613\n",
      "Epoch 7, training_accuracy: 0.7658833333333334,\t validation accuracy: 0.7761\n",
      "Epoch 8, training_accuracy: 0.7694,\t validation accuracy: 0.7817\n",
      "Epoch 9, training_accuracy: 0.7753333333333333,\t validation accuracy: 0.7821\n",
      "Epoch 10, training_accuracy: 0.7921333333333334,\t validation accuracy: 0.7996\n",
      "Epoch 11, training_accuracy: 0.7964333333333333,\t validation accuracy: 0.8049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-35a437361891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_DNN_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-123a08a2229c>\u001b[0m in \u001b[0;36mtrain_DNN_minibatch\u001b[0;34m(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test, y_test)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             print(\"Epoch {0}, training_accuracy: {1},\\t validation accuracy: {2}\".\n\u001b[0;32m---> 20\u001b[0;31m                   format(epoch + 1, evaluate(X_train, y_train, network), evaluate(X_test, y_test, network)))\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             print(\"Epoch {0}, training_accuracy: {1}\".\n",
      "\u001b[0;32m<ipython-input-17-123a08a2229c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_val, y_val, network)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-123a08a2229c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[784, 30, 10], activation=\"relu\")\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.005, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import urllib.request\n",
    "import os, tarfile\n",
    "import pickle, sys, random\n",
    "import gzip, time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'labels', b'data', b'batch_label', b'filenames'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class standardscale:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        self.mean = np.mean(X, axis=0)   # .astype(np.float32)\n",
    "        self.std = np.std(X, axis=0)   # .astype(np.float32)\n",
    "        return (X - self.mean) / self.std\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "for i in range(1, 6):\n",
    "    with open('cifar10/data_batch_%d' % i, 'rb') as f:\n",
    "        whole = pickle.load(f, encoding='bytes')\n",
    "        data.extend(whole[b'data'])\n",
    "        labels.extend(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = [], []\n",
    "with open('cifar10/test_batch', 'rb') as f:\n",
    "    whole = pickle.load(f, encoding='bytes')\n",
    "    test_data = whole[b'data']\n",
    "    test_labels = np.array(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train = np.array(data) / 255.0\n",
    "# X_test = np.array(test_data) / 255.0\n",
    "# y_train = np.eye(10)[np.array(labels, dtype=np.int32)]\n",
    "# y_test = np.eye(10)[np.array(test_labels, dtype=np.int32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(iinfo(min=-32768, max=32767, dtype=int16),\n",
       " finfo(resolution=1e-06, min=-3.4028235e+38, max=3.4028235e+38, dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.iinfo(np.int16), np.finfo(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array(data).astype(np.float32)\n",
    "X_test = np.array(test_data).astype(np.float32)\n",
    "y_train = np.eye(10)[np.array(labels, dtype=np.int32)]\n",
    "y_test = np.eye(10)[np.array(test_labels, dtype=np.int32)]\n",
    "\n",
    "# ss = StandardScaler()\n",
    "# X_train = ss.fit_transform(X_train)\n",
    "# X_test = ss.transform(X_test)\n",
    "\n",
    "ss = standardscale()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585.9376068115234, 3.8148040771484375)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(X_train) / 1024 / 1024, sys.getsizeof(y_train) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = [np.reshape(row, (3072, 1)) for row in data]\n",
    "# X_test = [np.reshape(row, (3072, 1)) for row in test_data]\n",
    "# two_dim = np.eye(10)[np.array(labels, dtype=np.int32)]\n",
    "# two_dim_test = np.eye(10)[np.array(test_labels, dtype=np.int32)]\n",
    "# y_train = [row.reshape(10, 1) for row in two_dim]\n",
    "# y_test = [row.reshape(10, 1) for row in two_dim_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\", dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param sizes: list of layers\n",
    "        :param activations: activation_functions\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer)  * np.sqrt(2.0 / forward_layer) \n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(back_layer, 1) for back_layer in sizes[1:]]\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # TODO  activation_functions = {'sigmoid': sigmoid, 'relu': relu} tanh\n",
    "        if activation.lower() == \"sigmoid\":\n",
    "            self.activation = Network.sigmoid\n",
    "            self.activation_derivative = Network.sigmoid_derivative\n",
    "        elif activation.lower() == \"relu\":\n",
    "            self.activation = Network.relu\n",
    "            self.activation_derivative = Network.relu_derivative\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(w, a) + b)\n",
    "            a *= (1.0 - self.dropout_rate)  ######### test dropout\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # forward pass #\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "\n",
    "            self.mask = np.random.rand(*z.shape) > self.dropout_rate\n",
    "            z *= self.mask\n",
    "        #    z /= (1 - self.dropout_rate)\n",
    "\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(Network.softmax(final_layer))\n",
    "\n",
    "        # backward pass#\n",
    "        delta = Network.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l - 1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_w, gradient_b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = z - np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_batch(z):\n",
    "        z = z.T\n",
    "        z = z - np.max(z, axis=0)\n",
    "        t = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        return t.T\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(a, b):\n",
    "        return a - b\n",
    "\n",
    "\n",
    "class Network_mini_batch(Network):\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\"):\n",
    "        super().__init__(sizes, activation)\n",
    "        self.weights = [np.random.randn(forward_layer, back_layer) * np.sqrt(2.0 / forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer) for layer in sizes[1:]]\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(a, w) + b)\n",
    "        a = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(a, w) + b  # batch  z = a * w + b\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(self.softmax_batch(final_layer))\n",
    "        \n",
    "        delta = self.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(a_hold[-2].T, delta)\n",
    "        gradient_b[-1] = np.sum(delta, axis=0)\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(delta, self.weights[-l + 1].T) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(a_hold[-l - 1].T, delta)\n",
    "            gradient_b[-l] = np.sum(delta, axis=0)\n",
    "\n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_DNN_minibatch(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test=None, y_test=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        random_mask = np.random.choice(len(X_train), len(X_train), replace=False)\n",
    "        X_train = X_train[random_mask]\n",
    "        y_train = y_train[random_mask]\n",
    "\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i : i + batch_size]\n",
    "            y_batch = y_train[i : i + batch_size]\n",
    "        #    gradient_w = [np.zeros(w.shape) for w in network.weights]\n",
    "        #    gradient_b = [np.zeros(b.shape) for b in network.biases]\n",
    "            add_w, add_b = network.backprop(X_batch, y_batch)\n",
    "        #    gradient_w = [gw + aw for gw, aw in zip(gradient_w, add_w)]\n",
    "        #    gradient_b = [gb + ab for gb, ab in zip(gradient_b, add_b)]\n",
    "        #    network.weights = [weight - learning_rate * gw / batch_size for weight, gw in zip(network.weights, gradient_w)]\n",
    "        #    network.biases = [bias - learning_rate * gb / batch_size for bias, gb in zip(network.biases, gradient_b)]\n",
    "\n",
    "            network.weights = [weight - learning_rate * gw / batch_size for weight, gw in zip(network.weights, add_w)]\n",
    "            network.biases = [bias - learning_rate * gb / batch_size for bias, gb in zip(network.biases, add_b)]\n",
    "            \n",
    "        if X_test is not None:\n",
    "            print(\"Epoch {}, training_accuracy: {:>6},  validation accuracy: {:>6},  epoch time: {:.2f}s\".format(\n",
    "                  epoch + 1, \n",
    "                  evaluate(X_train, y_train, network), \n",
    "                  evaluate(X_test, y_test, network), \n",
    "                  time.time() - start))\n",
    "        else:\n",
    "            print(\"Epoch {0}, training_accuracy: {1}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network)))\n",
    "\n",
    "def evaluate(X_val, y_val, network):\n",
    "    y_pred = [np.argmax(network.predict(x)) for x in X_val]\n",
    "    return np.mean([int(y_p == np.argmax(y)) for y_p, y in zip(y_pred, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.29736,  validation accuracy: 0.2978,  epoch time: 13.59s\n",
      "Epoch 2, training_accuracy: 0.33598,  validation accuracy: 0.3335,  epoch time: 12.80s\n",
      "Epoch 3, training_accuracy: 0.35918,  validation accuracy: 0.3543,  epoch time: 11.79s\n",
      "Epoch 4, training_accuracy: 0.36418,  validation accuracy: 0.3584,  epoch time: 11.24s\n",
      "Epoch 5, training_accuracy:  0.383,  validation accuracy: 0.3718,  epoch time: 12.83s\n",
      "Epoch 6, training_accuracy: 0.38762,  validation accuracy: 0.3756,  epoch time: 15.84s\n",
      "Epoch 7, training_accuracy: 0.39564,  validation accuracy: 0.3871,  epoch time: 16.17s\n",
      "Epoch 8, training_accuracy: 0.40156,  validation accuracy: 0.3906,  epoch time: 11.76s\n",
      "Epoch 9, training_accuracy: 0.40632,  validation accuracy: 0.3933,  epoch time: 12.26s\n",
      "Epoch 10, training_accuracy: 0.40862,  validation accuracy: 0.3952,  epoch time: 13.12s\n",
      "Epoch 11, training_accuracy: 0.41696,  validation accuracy:  0.402,  epoch time: 14.71s\n",
      "Epoch 12, training_accuracy: 0.4181,  validation accuracy: 0.4102,  epoch time: 28.97s\n",
      "Epoch 13, training_accuracy: 0.42466,  validation accuracy: 0.4145,  epoch time: 23.89s\n",
      "Epoch 14, training_accuracy: 0.4292,  validation accuracy: 0.4184,  epoch time: 14.17s\n",
      "Epoch 15, training_accuracy: 0.42822,  validation accuracy: 0.4173,  epoch time: 12.18s\n",
      "Epoch 16, training_accuracy: 0.43162,  validation accuracy: 0.4168,  epoch time: 14.88s\n",
      "Epoch 17, training_accuracy: 0.43598,  validation accuracy: 0.4226,  epoch time: 16.14s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa298e06514a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# , dropout_rate=0.5  / 255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_DNN_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f9d6e135341e>\u001b[0m in \u001b[0;36mtrain_DNN_minibatch\u001b[0;34m(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test, y_test)\u001b[0m\n\u001b[1;32m     20\u001b[0m             print(\"Epoch {}, training_accuracy: {:>6},  validation accuracy: {:>6},  epoch time: {:.2f}s\".format(\n\u001b[1;32m     21\u001b[0m                   \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                   \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                   \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                   time.time() - start))\n",
      "\u001b[0;32m<ipython-input-7-f9d6e135341e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_val, y_val, network)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f9d6e135341e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ddc120c76099>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[3072, 50, 10], activation=\"relu\")   # , dropout_rate=0.5  / 255.0\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.001, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.37208,  validation accuracy: 0.3695,  epoch time: 10.94s\n",
      "Epoch 2, training_accuracy: 0.41092,  validation accuracy: 0.4089,  epoch time: 14.32s\n",
      "Epoch 3, training_accuracy: 0.43184,  validation accuracy: 0.4226,  epoch time: 13.79s\n",
      "Epoch 4, training_accuracy: 0.44864,  validation accuracy: 0.4357,  epoch time: 14.46s\n",
      "Epoch 5, training_accuracy: 0.45824,  validation accuracy: 0.4426,  epoch time: 11.38s\n",
      "Epoch 6, training_accuracy: 0.47076,  validation accuracy: 0.4534,  epoch time: 15.47s\n",
      "Epoch 7, training_accuracy: 0.47942,  validation accuracy: 0.4587,  epoch time: 12.56s\n",
      "Epoch 8, training_accuracy: 0.48462,  validation accuracy: 0.4607,  epoch time: 16.80s\n",
      "Epoch 9, training_accuracy: 0.4916,  validation accuracy: 0.4634,  epoch time: 10.64s\n",
      "Epoch 10, training_accuracy: 0.49618,  validation accuracy: 0.4628,  epoch time: 13.11s\n",
      "Epoch 11, training_accuracy: 0.50256,  validation accuracy: 0.4713,  epoch time: 10.54s\n",
      "Epoch 12, training_accuracy: 0.50498,  validation accuracy: 0.4681,  epoch time: 13.77s\n",
      "Epoch 13, training_accuracy: 0.51096,  validation accuracy: 0.4743,  epoch time: 15.32s\n",
      "Epoch 14, training_accuracy: 0.5143,  validation accuracy: 0.4763,  epoch time: 15.85s\n",
      "Epoch 15, training_accuracy: 0.51798,  validation accuracy: 0.4738,  epoch time: 12.11s\n",
      "Epoch 16, training_accuracy: 0.52094,  validation accuracy: 0.4795,  epoch time: 19.06s\n",
      "Epoch 17, training_accuracy: 0.52224,  validation accuracy: 0.4786,  epoch time: 13.72s\n",
      "Epoch 18, training_accuracy: 0.52768,  validation accuracy: 0.4788,  epoch time: 16.72s\n",
      "Epoch 19, training_accuracy: 0.5288,  validation accuracy: 0.4801,  epoch time: 11.69s\n",
      "Epoch 20, training_accuracy: 0.53378,  validation accuracy: 0.4822,  epoch time: 12.83s\n",
      "Epoch 21, training_accuracy: 0.53608,  validation accuracy:  0.481,  epoch time: 11.19s\n",
      "Epoch 22, training_accuracy: 0.53786,  validation accuracy: 0.4831,  epoch time: 11.80s\n",
      "Epoch 23, training_accuracy: 0.5419,  validation accuracy: 0.4839,  epoch time: 12.13s\n",
      "Epoch 24, training_accuracy: 0.54262,  validation accuracy: 0.4872,  epoch time: 12.76s\n",
      "Epoch 25, training_accuracy: 0.54414,  validation accuracy: 0.4863,  epoch time: 10.79s\n",
      "Epoch 26, training_accuracy: 0.5458,  validation accuracy: 0.4855,  epoch time: 12.00s\n",
      "Epoch 27, training_accuracy: 0.55066,  validation accuracy: 0.4849,  epoch time: 10.88s\n",
      "Epoch 28, training_accuracy: 0.55256,  validation accuracy: 0.4876,  epoch time: 11.82s\n",
      "Epoch 29, training_accuracy: 0.5537,  validation accuracy: 0.4854,  epoch time: 10.86s\n",
      "Epoch 30, training_accuracy: 0.55538,  validation accuracy: 0.4884,  epoch time: 13.66s\n"
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[3072, 50, 10], activation=\"relu\")   # standardscaler lr=1e-3 batch=32 he_initialization\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.001, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.47472,  validation accuracy: 0.4514,  epoch time: 110.88s\n",
      "Epoch 2, training_accuracy: 0.49816,  validation accuracy: 0.4606,  epoch time: 101.24s\n",
      "Epoch 3, training_accuracy: 0.51818,  validation accuracy: 0.4761,  epoch time: 116.53s\n",
      "Epoch 4, training_accuracy: 0.52858,  validation accuracy:  0.473,  epoch time: 106.83s\n",
      "Epoch 5, training_accuracy: 0.53798,  validation accuracy: 0.4799,  epoch time: 97.94s\n",
      "Epoch 6, training_accuracy: 0.5605,  validation accuracy: 0.4982,  epoch time: 98.75s\n",
      "Epoch 7, training_accuracy: 0.55068,  validation accuracy: 0.4841,  epoch time: 113.24s\n",
      "Epoch 8, training_accuracy: 0.56852,  validation accuracy: 0.4883,  epoch time: 100.15s\n",
      "Epoch 9, training_accuracy: 0.5784,  validation accuracy: 0.4966,  epoch time: 93.72s\n",
      "Epoch 10, training_accuracy: 0.57538,  validation accuracy: 0.4857,  epoch time: 97.17s\n",
      "Epoch 11, training_accuracy: 0.58556,  validation accuracy: 0.4979,  epoch time: 98.52s\n",
      "Epoch 12, training_accuracy: 0.58198,  validation accuracy: 0.4846,  epoch time: 93.49s\n",
      "Epoch 13, training_accuracy: 0.59494,  validation accuracy: 0.4935,  epoch time: 96.44s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5dce7431327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# , dropout_rate=0.5   standardscaler batch=2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_DNN_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f9d6e135341e>\u001b[0m in \u001b[0;36mtrain_DNN_minibatch\u001b[0;34m(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test, y_test)\u001b[0m\n\u001b[1;32m     20\u001b[0m             print(\"Epoch {}, training_accuracy: {:>6},  validation accuracy: {:>6},  epoch time: {:.2f}s\".format(\n\u001b[1;32m     21\u001b[0m                   \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                   \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                   \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                   time.time() - start))\n",
      "\u001b[0;32m<ipython-input-6-f9d6e135341e>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_val, y_val, network)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f9d6e135341e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ddc120c76099>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[3072, 50, 10], activation=\"relu\")   # , dropout_rate=0.5   standardscaler batch=2\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.001, 2, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training_accuracy: 0.41644,  validation accuracy: 0.4038,  epoch time: 287.32s\n",
      "Epoch 2, training_accuracy: 0.46092,  validation accuracy: 0.4347,  epoch time: 245.60s\n",
      "Epoch 3, training_accuracy: 0.4903,  validation accuracy: 0.4519,  epoch time: 244.37s\n",
      "Epoch 4, training_accuracy: 0.5083,  validation accuracy: 0.4693,  epoch time: 244.65s\n",
      "Epoch 5, training_accuracy: 0.52918,  validation accuracy:  0.482,  epoch time: 247.81s\n",
      "Epoch 6, training_accuracy: 0.54614,  validation accuracy: 0.4875,  epoch time: 244.36s\n",
      "Epoch 7, training_accuracy: 0.5569,  validation accuracy: 0.4959,  epoch time: 238.25s\n",
      "Epoch 8, training_accuracy: 0.57158,  validation accuracy: 0.4959,  epoch time: 235.04s\n",
      "Epoch 9, training_accuracy: 0.5887,  validation accuracy: 0.5019,  epoch time: 235.43s\n",
      "Epoch 10, training_accuracy: 0.60128,  validation accuracy: 0.5103,  epoch time: 239.37s\n",
      "Epoch 11, training_accuracy: 0.60844,  validation accuracy: 0.5086,  epoch time: 240.70s\n",
      "Epoch 12, training_accuracy: 0.62564,  validation accuracy: 0.5175,  epoch time: 236.95s\n",
      "Epoch 13, training_accuracy: 0.63392,  validation accuracy: 0.5119,  epoch time: 2907.76s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-14d7ff12c8ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# , dropout_rate=0.5   standardscaler lr=1e-3 batch=32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_DNN_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f9d6e135341e>\u001b[0m in \u001b[0;36mtrain_DNN_minibatch\u001b[0;34m(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test, y_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mgradient_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0madd_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mgradient_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mab\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mab\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ddc120c76099>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mz_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m  \u001b[1;31m# batch  z = a * w + b\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mz_hold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[3072, 1024, 512, 100, 10], activation=\"relu\")   # , dropout_rate=0.5   standardscaler lr=1e-3 batch=32\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.001, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8cc6d0056978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork_mini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3072\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# , dropout_rate=0.5   standardscaler lr=1e-3 batch=32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_DNN_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f9d6e135341e>\u001b[0m in \u001b[0;36mtrain_DNN_minibatch\u001b[0;34m(X_train, y_train, num_epochs, learning_rate, batch_size, network, X_test, y_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mgradient_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0madd_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mgradient_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mgradient_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mab\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mab\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ddc120c76099>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_hold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mgradient_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_hold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mgradient_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dnn = Network_mini_batch(sizes=[3072, 8000, 4000, 2000, 500, 10], activation=\"relu\")   # , dropout_rate=0.5   standardscaler lr=1e-3 batch=32\n",
    "train_DNN_minibatch(X_train, y_train, 30, 0.001, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam & Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import urllib.request\n",
    "import os\n",
    "import pickle, sys, random\n",
    "import gzip, time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels = [], []\n",
    "for i in range(1, 6):\n",
    "    with open('cifar10/data_batch_%d' % i, 'rb') as f:\n",
    "        whole = pickle.load(f, encoding='bytes')\n",
    "        data.extend(whole[b'data'])\n",
    "        labels.extend(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data, test_labels = [], []\n",
    "with open('cifar10/test_batch', 'rb') as f:\n",
    "    whole = pickle.load(f, encoding='bytes')\n",
    "    test_data = whole[b'data']\n",
    "    test_labels = np.array(whole[b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = np.array(data) \n",
    "X_test = np.array(test_data)\n",
    "y_train = np.eye(10)[np.array(labels, dtype=np.int32)]\n",
    "y_test = np.eye(10)[np.array(test_labels, dtype=np.int32)]\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\", dropout_rate=0.0):\n",
    "        \"\"\"\n",
    "        :param sizes: list of layers\n",
    "        :param activations: activation_functions\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(back_layer, forward_layer)  * np.sqrt(2.0 / forward_layer) \n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(back_layer, 1) for back_layer in sizes[1:]]\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # TODO  activation_functions = {'sigmoid': sigmoid, 'relu': relu} tanh\n",
    "        if activation.lower() == \"sigmoid\":\n",
    "            self.activation = Network.sigmoid\n",
    "            self.activation_derivative = Network.sigmoid_derivative\n",
    "        elif activation.lower() == \"relu\":\n",
    "            self.activation = Network.relu\n",
    "            self.activation_derivative = Network.relu_derivative\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(w, a) + b)\n",
    "            a *= (1.0 - self.dropout_rate)  ######### test dropout\n",
    "        a = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        # forward pass #\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(w, a) + b\n",
    "\n",
    "            self.mask = np.random.rand(*z.shape) > self.dropout_rate\n",
    "            z *= self.mask\n",
    "        #    z /= (1 - self.dropout_rate)\n",
    "\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(self.weights[-1], a) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(Network.softmax(final_layer))\n",
    "\n",
    "        # backward pass#\n",
    "        delta = Network.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(delta, a_hold[-2].T)\n",
    "        gradient_b[-1] = delta\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(self.weights[-l + 1].T, delta) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(delta, a_hold[-l - 1].T)\n",
    "            gradient_b[-l] = delta\n",
    "\n",
    "        return gradient_w, gradient_b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        return Network.sigmoid(z) * (1 - Network.sigmoid(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        mask = (z <= 0)\n",
    "        dout = np.ones(z.shape)\n",
    "        dout[mask] = 0.0\n",
    "        return dout\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        z = z - np.max(z)\n",
    "        return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_batch(z):\n",
    "        z = z.T\n",
    "        z = z - np.max(z, axis=0)\n",
    "        t = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        return t.T\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(a, b):\n",
    "        return a - b\n",
    "\n",
    "\n",
    "class Network_mini_batch(Network):\n",
    "    def __init__(self, sizes=[100, 100], activation=\"relu\"):\n",
    "        super().__init__(sizes, activation)\n",
    "        self.weights = [np.random.randn(forward_layer, back_layer) * np.sqrt(2.0 / forward_layer) \\\n",
    "                        for forward_layer, back_layer in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(layer) for layer in sizes[1:]]\n",
    "\n",
    "    def predict(self, a):\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = self.activation(np.dot(a, w) + b)\n",
    "        a = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        return a\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "        a = x\n",
    "        a_hold = [x]\n",
    "        z_hold = []\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            z = np.dot(a, w) + b  # batch  z = a * w + b\n",
    "            a = self.activation(z)\n",
    "            z_hold.append(z)\n",
    "            a_hold.append(a)\n",
    "        final_layer = np.dot(a, self.weights[-1]) + self.biases[-1]\n",
    "        z_hold.append(final_layer)\n",
    "        a_hold.append(self.softmax_batch(final_layer))\n",
    "        \n",
    "        delta = self.softmax_derivative(a_hold[-1], y)\n",
    "        gradient_w[-1] = np.dot(a_hold[-2].T, delta)\n",
    "        gradient_b[-1] = np.sum(delta, axis=0)\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            delta = np.dot(delta, self.weights[-l + 1].T) * self.activation_derivative(z_hold[-l])\n",
    "            gradient_w[-l] = np.dot(a_hold[-l - 1].T, delta)\n",
    "            gradient_b[-l] = np.sum(delta, axis=0)\n",
    "\n",
    "        return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr, momentum, batch_size):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.batch_size = batch_size\n",
    "        self.velocity = None\n",
    "\n",
    "    def update(self, weights, biases, grad_w, grad_b):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = dict()\n",
    "            self.velocity['w'] = [np.zeros(w.shape) for w in weights]\n",
    "            self.velocity['b'] = [np.zeros(b.shape) for b in biases]\n",
    "\n",
    "        self.velocity['w'] = [self.momentum * vw - self.lr * gw / self.batch_size\n",
    "                              for vw, gw in zip(self.velocity['w'], grad_w)]\n",
    "        self.velocity['b'] = [self.momentum * vb - self.lr * gb / self.batch_size\n",
    "                              for vb, gb in zip(self.velocity['b'], grad_b)]\n",
    "\n",
    "    #    weights = [w + vw for w, vw in zip(weights, self.velocity['w'])].copy()\n",
    "    #    biases = [b + vb for b, vb in zip(biases, self.velocity['b'])].copy()\n",
    "\n",
    "        for i, (w, vw) in enumerate(zip(weights, self.velocity['w'])):\n",
    "            weights[i] = w + vw\n",
    "\n",
    "        for i, (b, vb) in enumerate(zip(biases, self.velocity['b'])):\n",
    "            biases[i] = b + vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, lr, batch_size, rho1=0.9, rho2=0.999):\n",
    "        self.lr = lr\n",
    "        self.rho1 = rho1\n",
    "        self.rho2 = rho2\n",
    "        self.batch_size = batch_size\n",
    "        self.iteration = 0\n",
    "        self.ps = None\n",
    "\n",
    "    def update(self, weights, biases, grad_w, grad_b):\n",
    "        self.iteration += 1\n",
    "        grad_w = [gw / self.batch_size for gw in grad_w]\n",
    "        grad_b = [gb / self.batch_size for gb in grad_b]\n",
    "\n",
    "        if self.ps is None:\n",
    "            self.ps = {}\n",
    "            self.ps['vw'] = [np.zeros(w.shape) for w in weights]\n",
    "            self.ps['vb'] = [np.zeros(b.shape) for b in biases]\n",
    "            self.ps['hw'] = [np.zeros(w.shape) for w in weights]\n",
    "            self.ps['hb'] = [np.zeros(b.shape) for b in biases]\n",
    "\n",
    "        self.ps['vw'] = [self.rho1 * vw + (1 - self.rho1) * gw for vw, gw in zip(self.ps['vw'], grad_w)]\n",
    "        self.ps['vb'] = [self.rho1 * vb + (1 - self.rho1) * gb for vb, gb in zip(self.ps['vb'], grad_b)]\n",
    "        self.ps['hw'] = [self.rho2 * hw + (1 - self.rho2) * (gw ** 2) for hw, gw in zip(self.ps['hw'], grad_w)]\n",
    "        self.ps['hb'] = [self.rho2 * hb + (1 - self.rho2) * (gb ** 2) for hb, gb in zip(self.ps['hb'], grad_b)]\n",
    "        unbias_vw = [vw / (1 - self.rho1 ** self.iteration) for vw in self.ps['vw']]\n",
    "        unbias_vb = [vb / (1 - self.rho1 ** self.iteration) for vb in self.ps['vb']]\n",
    "        unbias_hw = [hw / (1 - self.rho2 ** self.iteration) for hw in self.ps['hw']]\n",
    "        unbias_hb = [hb / (1 - self.rho2 ** self.iteration) for hb in self.ps['hb']]\n",
    "\n",
    "        for i, (w, vw, hw) in enumerate(zip(weights, unbias_vw, unbias_hw)):\n",
    "            weights[i] = w - self.lr * vw / (np.sqrt(hw) + 1e-8)\n",
    "\n",
    "        for i, (b, vb, hb) in enumerate(zip(biases, unbias_vb, unbias_hb)):\n",
    "            biases[i] = b - self.lr * vb / (np.sqrt(hb) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_DNN_minibatch(X_train, y_train, num_epochs, optimizer, batch_size, network, X_test=None, y_test=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        random_mask = np.random.choice(len(X_train), len(X_train), replace=False)\n",
    "        X_train = X_train[random_mask]\n",
    "        y_train = y_train[random_mask]\n",
    "\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i : i + batch_size]\n",
    "            y_batch = y_train[i : i + batch_size]\n",
    "    #        gradient_w = [np.zeros(w.shape) for w in network.weights]\n",
    "    #        gradient_b = [np.zeros(b.shape) for b in network.biases]\n",
    "            add_w, add_b = network.backprop(X_batch, y_batch)\n",
    "    #        gradient_w = [gw + aw for gw, aw in zip(gradient_w, add_w)]\n",
    "    #        gradient_b = [gb + ab for gb, ab in zip(gradient_b, add_b)]\n",
    "            \n",
    "            optimizer.update(network.weights, network.biases, add_w, add_b)   # gradient_w, gradient_b, add_w, add_b\n",
    "    #        pdb.set_trace()\n",
    "    #      network.weights = [weight - learning_rate * gw / batch_size for weight, gw in zip(network.weights, gradient_w)]\n",
    "    #       network.biases = [bias - learning_rate * gb / batch_size for bias, gb in zip(network.biases, gradient_b)]\n",
    "\n",
    "        if X_test is not None:\n",
    "            print(\"Epoch {}, training_accuracy: {:>6},  validation accuracy: {:>6},  epoch time: {:.2f}s\".format(\n",
    "                  epoch + 1, \n",
    "                  evaluate(X_train, y_train, network), \n",
    "                  evaluate(X_test, y_test, network), \n",
    "                  time.time() - start))\n",
    "        else:\n",
    "            print(\"Epoch {0}, training_accuracy: {1}\".\n",
    "                  format(epoch + 1, evaluate(X_train, y_train, network)))\n",
    "\n",
    "def evaluate(X_val, y_val, network):\n",
    "    y_pred = [np.argmax(network.predict(x)) for x in X_val]\n",
    "    return np.mean([int(y_p == np.argmax(y)) for y_p, y in zip(y_pred, y_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pdb.set_trace()\n",
    "dnn = Network_mini_batch(sizes=[3072, 50, 10], activation=\"relu\")   # standardscaler lr=1e-3 batch=32 he_initialization\n",
    "optimizer = Momentum(lr=0.001, momentum=0.9, batch_size=32)\n",
    "train_DNN_minibatch(X_train, y_train, 30, optimizer, 32, dnn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
